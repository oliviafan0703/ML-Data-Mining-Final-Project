
---
title: "Final Project Report"
author: "Olivia Fan, Alicia Gong"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, message=F, warning=F, echo=F}
library(knitr)
library(dplyr)
library(tidyverse)
library(e1071)
library(randomForest)
library(caret)
library(ggplot2)
library(psych)
library(BioStatR)
```

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

# Introduction

Breast cancer is the most common cancer worldwide and the most common cancer diagnosed in the US (Mayo). Each year in the US, about 264,000 cases of breast cancer are diagnosed in women and about 2,400 in men (CDC). 

Early diagnosis of the condition is crucial to improve the survival rate and relieve suffering in patients. Mammography is an effective X-ray imaging technology that detects breast cancer early. Classically, benign or malignant breast tumors are diagnosed by radiologists' interpretation of mammograms based on clinical parameters. However, diagnosing cancer is challenging even for the most skilled doctors. Since masses are heterogeneous, clinical parameters supply limited information on mammography mass. The symptoms are often shared with diseases and conditions that are unrelated to cancer, leading doctors to improperly diagnose the disease. 

Cancerous lumps are often confused for blocked milk ducts, breast cysts, and other benign conditions. According to an expansive study conducted by Dartmouth College, the University of Vermont, and the Fred Hutchinson Cancer Research Center, and published in the March 2015 issue of the Journal of American Medical Association, approximately 13% of the diagnoses missed Stage 1 breast cancer. Meanwhile, 48% failed to detect atypia hyperplasia, a precursor to breast cancer. A significant number also over-diagnosed atypia hyperplasia. 

There is, therefore, an urgent need to find new tools that can identify patients with breast cancer. Our study aims to build supervised machine-learning models to predict the diagnosis of breast cancer and understand the most important variables, to assist doctors and radiologists in accurately interpreting mammography imaging. 

We built 3 models in total: Lasso penalized logistic regression, SVM, and Random Forest.

# Data

We obtain the Breast Cancer Wisconsin (Diagnosis) Data Set from Kaggle. The dataset contains diagnosis results and features of the cell nuclei computed from a digitized image of a fine needle aspirate (FNA) of a breast mass for 568 patients. The size of the nucleus is expressed by the features radius and area. The shape is expressed by the features smoothness, concavity, compactness, concave points, symmetry, and fractal dimension. The perimeter expresses both the size and shape of the nucleus. A higher value of shape features corresponds to a less regular contour and, therefore, to a higher probability of malignancy. For each of the features the mean value, worst value (mean of the three largest values),
and standard error are computed for each image, resulting in 30 features of
568 images.

# Data Processing

The original dataset contains a blank column '...33', so we dropped it. We also dropped the 'id' column, and rename several columns that contains blank space in their names. 

```{r load-data, message = FALSE, echo=F}
cancer<-read_csv("data.csv")%>%
  select(-...33)%>%
  select(-id)%>%
  rename(concave_points_mean='concave points_mean')%>%
  rename(concave_points_worst='concave points_worst')%>%
  rename(concave_points_se='concave points_se'
         )
```


In order to fit SVM on the data, we encode the `diagnosis` variable into a factor variable with level 1 and -1:

```{r}
cancer_binary<-cancer%>%
  mutate(diagnosis_binary=as.factor(ifelse(diagnosis=="M",1,-1)))
```

We partition the data into training and testing sets using a 70-30 percentage split(70% of the original data as the training set, and 30% as the testing set):

```{r}
set.seed(123)
dt = sort(sample(nrow(cancer_binary), nrow(cancer_binary)*.7))
cancer_train<-cancer_binary[dt,]
cancer_test<-cancer_binary[-dt,]
```

# EDA
```{r}
ggplot(data = cancer, mapping = aes(x = diagnosis)) +
  geom_bar()
```
The bar plot shows that there is a larger number of benign than malignant cancer. 

We divide the data into 3 categories according to their features.
```{r}
features_mean= cancer[2:10]
features_se= cancer[11:20]
features_worst= cancer[21:31]
```

```{r}
library(corrplot)
corrplot(cor(features_mean))
```

Major observations: 

- Radius_mean, perimeter_mean, and area_mean are highly correlated.

- Compactness_mean, concavity_mean and concave_points_mean are highly correlated.


```{r, warning=FALSE}
panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, color = "dodgerblue3", ...)
}

pairs(cancer[2:10], 
             method = "pearson", # correlation method
             digits=3,
             pch = '.',
             col=ifelse(cancer$diagnosis=="M", "red", "blue"),
             density = TRUE,  # show density plots
             ellipses = TRUE, # show correlation ellipses
             diag.panel =  panel.hist
             )
```

# Methodology 

## Model 1: LASSO-Penalized Logistic Regression

### Model Selection

```{r}
# Dumy code categorical predictor variables
x <- model.matrix(diagnosis~.-diagnosis_binary, cancer_train)[,-1]
# Convert the outcome (class) to a numerical variable
y <- ifelse(cancer_train$diagnosis == "M", 1, 0)
```


```{r}
library(glmnet)
# Find the best lambda using cross-validation
set.seed(123)
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
plot(cv.lasso)
```

```{r}
cv.lasso$lambda.min
# Final model with lambda.min
lasso.model <- glmnet(x, y, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.min)
coef(lasso.model)
```

### Prediction

Using the logistic regression model, besides classification we also want to understand uncertainty - more specifically, predictive probabilities

```{r}
# Make predictions on the test data
x.test <- model.matrix(diagnosis~.-diagnosis_binary, cancer_test)[,-1]
probabilities <- lasso.model %>% predict(newx = x.test, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "M", "B")

#Present the prediction
predictors<-cancer_test%>%select(texture_mean,concave_points_mean,radius_se,texture_se,area_se,smoothness_se,compactness_se,fractal_dimension_se,radius_worst,texture_worst,smoothness_worst,concavity_worst,concave_points_worst,symmetry_worst)
predicted_results<-cbind(predictors,probabilities)%>%
  rename(probabilities=s0)
predicted_results<-cbind(predicted_results,predicted.classes)%>%
  rename("predicted_class"=s0)
predicted_results%>%kable(digits=3)
# Model accuracy
observed.classes <- cancer_test$diagnosis
mean(predicted.classes == observed.classes)
```

We achieved a prediction accuracy of 0.9766. To interpret the predictions, we see that a patient with tumor with `texture_mean` of 10.38, `concave_points_mean` of 0.147, `radius_se` of 1.095, `texture_se` of 0.905, `area_se` of 153.400, `smoothness_se` of 0.006, `compactness_se` of 0.049, `fractal_dimension_se` of 0.006, `radius_worst` of 25.380, `texture_worst` of 17.33, `smoothness_worst` of 0.162, `concavity_worst` of 0.712, `concave_points_worst` of 0.265, `symmetry_worst` of 0.460 is expected to have a 100% of being diagnosed as malignant tumor. 
Whereas... is expected to have a 0% of being diagnosed as malignant tumor. 
Whereas... is expected to have a 87.4% of being diagnosed as malignant tumor. 

## Model 1.2: Logistic Regression with Interaction Terms

```{r}
logistic_interaction <- glm(diagnosis_binary~texture_mean+concave_points_mean+radius_se+texture_se+area_se+smoothness_se+compactness_se+fractal_dimension_se+radius_worst+texture_worst+smoothness_worst+concavity_worst+concave_points_worst+symmetry_worst, data = cancer_train, family = "binomial")
summary(logistic_interaction)
```

## Model 2: SVM

### Linear Kernel SVM

We use the predictors selected by the LASSO penalized logistic regression as predictors for the support vector machine model:

If two predictors have high correlation, only use one of them:

```{r}
set.seed(1)
tune.out <- tune(svm, diagnosis_binary ~texture_mean+concave_points_mean+radius_se+texture_se+area_se+smoothness_se+compactness_se+fractal_dimension_se+radius_worst+texture_worst+smoothness_worst+concavity_worst+concave_points_worst+symmetry_worst, data = cancer_train, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1,
                                        1, 5, 10, 100)))
summary(tune.out)
best_linear_svm <- tune.out$best.model
summary(best_linear_svm)
```

```{r}
ypred <- predict(best_linear_svm, cancer_test)
table(predict = ypred, truth = cancer_test$diagnosis_binary)
classification_error <- 1- sum(ypred == cancer_test$diagnosis_binary)/length(ypred)
classification_error
```

The misclassification rate is 0.01754.


### Radial Kernel SVM

```{r}
set.seed(1)
tune.out <- tune(svm, diagnosis_binary ~ texture_mean+concave_points_mean+radius_se+texture_se+area_se+smoothness_se+compactness_se+fractal_dimension_se+radius_worst+texture_worst+smoothness_worst+concavity_worst+concave_points_worst+symmetry_worst, data = cancer_train,
                 kernel = "radial",
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)
best_radial_svm <- tune.out$best.model
summary(best_radial_svm)
```

```{r}
ypred <- predict(best_radial_svm, cancer_test)
table(predict = ypred, truth = cancer_test$diagnosis_binary)
classification_error <- 1- sum(ypred == cancer_test$diagnosis_binary)/length(ypred)
```

### VISUALIZATION FOR ORIGINAL MODEL

```{r}
plot(best_linear_svm, cancer_train, radius_se~concavity_mean)
plot(best_linear_svm, cancer_train, smoothness_se~concavity_mean)

plotpairs=function(fit){
  for (name in names(cancer_train)[!(names(cancer_train) %in% c("diagnosis", "diagnosis_binary"))]){
    plot(fit, cancer_train, as.formula(paste("radius_se~", name, sep=" ")))
  }
}

plotpairs(best_linear_svm)
```

### SVM Visualization

#### Linear

```{r, echo=FALSE}
tune.out <- tune(svm, diagnosis_binary ~ concavity_mean+radius_se+texture_se+smoothness_se, data = cancer_train, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1,
                                        1, 5, 10, 100)))
best_linear_graph <- tune.out$best.model
```

```{r}
plot(best_linear_graph, cancer_train, radius_se~concavity_mean)

plot(best_linear_graph, cancer_train, texture_se~concavity_mean)
```

### Radial

```{r, echo=FALSE}
tune.out <- tune(svm, diagnosis_binary ~ concavity_mean+radius_se+texture_se+smoothness_se,
                 kernel = "radial", data = cancer_train,
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))
best_radial_graph <- tune.out$best.model
```

```{r}
plot(best_radial_graph, cancer_train, texture_se~concavity_mean)
plot(best_radial_graph, cancer_train, concavity_mean~texture_se)
```


### ROC (Linear SVM)

On train data: 

```{r}
library(pROC)
fitted <- attributes(predict(best_linear_svm, cancer_train,
                             decision.values=TRUE))$decision.values
ROC_svm1 <- roc(cancer_train$diagnosis_binary, fitted)

fitted <- attributes(predict(best_radial_svm, cancer_train,
                             decision.values=TRUE))$decision.values
ROC_svm2 <- roc(cancer_train$diagnosis_binary, fitted)
plot(ROC_svm1, col = "blue")
lines(ROC_svm2, col = "red")
legend("bottomright", legend = c("Linear", "Radial"), 
       col = c("blue", "red"), lty = 1)
```

On test data: 

```{r}
fitted <- attributes(predict(best_linear_svm, cancer_test,
                             decision.values=TRUE))$decision.values
ROC_svm1 <- roc(cancer_test$diagnosis_binary, fitted)

fitted <- attributes(predict(best_radial_svm, cancer_test,
                             decision.values=TRUE))$decision.values
ROC_svm2 <- roc(cancer_test$diagnosis_binary, fitted)
plot(ROC_svm1, col = "blue")
lines(ROC_svm2, col = "red")
legend("bottomright", legend = c("Linear", "Radial"), 
       col = c("blue", "red"), lty = 1)
```

## Model 3: Random Forest

For Random Forest, we decided to use group 'Mean' and group 'Worst' separately. This is because, we want to understand the potential difference in prediction given the different level of severity of the patients' conditions. As we have mentioned in the introduction, 'Worst' measures mean of the three
largest values. By building a model with only 'Mean' and 'Worst' variables, we think we can better understand the extreme cases. 

First, We select variables by running a preliminary random forest model with all the variables, to rank their importance. 

```{r}
rf <- randomForest(diagnosis_binary~.-diagnosis, data=cancer_train, proximity=TRUE)
importance(rf)
```
After testing different variables based on their important, and taking into accounts the collinearity issue we discussed in the EDA section, we decided to select the following variables as the predictors:
concave_points_worst, area_worst, perimeter_worst, radius_worst, concave_points_mean, perimeter_mean, concavity_worst, area_se.

We chose mtry=2, because after tuning mtry, we found that mtry=2 has the lowest OOB error.
```{r}
set.seed(123)
rf_predictors <- cancer_train %>% select(concave_points_worst, area_worst, perimeter_worst, radius_worst, concave_points_mean, perimeter_mean, concavity_worst, area_se)
mtry <- tuneRF(mix_predictors, cancer_train$diagnosis_binary, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE, doBest = TRUE)
```
We chose the number of tree to be 500. The number of trees should be chosen carefully, since a high performance of the individual models might lead to overfitting when the number of trees is very high. However, taking 50 trees caused a lower accuracy than taking 500 trees. Therefore, this higher number
of trees is chosen. 

```{r}
set.seed(123)
rf_mix <- randomForest(diagnosis_binary~concave_points_worst+area_worst+perimeter_worst+radius_worst+concave_points_mean+perimeter_mean+concavity_worst+area_se, data = cancer_train, proximity=TRUE, mtry=2)
print(rf_mix)
```
```{r}
pred_mix <- predict(rf_mix, cancer_test)
confusionMatrix(pred_mix, cancer_test$diagnosis_binary)
```
The RF model yields a 96% accuracy for the testing set. 

```{r}
ImpDatamix <- as.data.frame(importance(rf_mix))
ImpDatamix$Var.Names <- row.names(ImpDatamix)

ggplot(ImpDatamix, aes(x=Var.Names, y= MeanDecreaseGini)) +
  geom_segment(aes(x=Var.Names, xend=Var.Names, y=0, yend=MeanDecreaseGini), color="skyblue") +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )
```
From the plot we can see that the most important predictors are: concave_points_worst, concave_points_mean, area_worst, perimeter_worst, and radius_worst. Interestingly, perimeter_worst has high gini coefficient, but perimeter_mean ranks second from the last.

# Conclusion

Concavity is the severity of concave portions of the contour. A high concavity means that the boundary of the cell nucleus has indentations, and thus is rather rough than smooth. Concave points id the number of concave portions of the contour of the cell nucleus.

