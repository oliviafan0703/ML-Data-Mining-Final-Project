
---
title: "Final Project Report"
author: "Olivia Fan, Alicia Gong"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, message=F, warning=F, echo=F}
library(knitr)
library(dplyr)
library(tidyverse)
library(e1071)
library(randomForest)
library(caret)
library(ggplot2)
library(psych)
library(BioStatR)
```

```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

# Data Processing

```{r load-data, message = FALSE, echo=F}
cancer<-read_csv("data.csv")%>%
  select(-...33)%>%
  select(-id)%>%
  rename(concave_points_mean='concave points_mean')%>%
  rename(concave_points_worst='concave points_worst')%>%
  rename(concave_points_se='concave points_se'
         )
```


In order to fit SVM on the data, we encode the `diagnosis` variable into a factor variable with level 1 and -1:


```{r}
cancer_binary<-cancer%>%
  mutate(diagnosis_binary=as.factor(ifelse(diagnosis=="M",1,-1)))
```

We partition the data into training and testing sets using a 70-30 percentage split(70% of the original data as the training set, and 30% as the testing set):

```{r}
set.seed(123)
dt = sort(sample(nrow(cancer_binary), nrow(cancer_binary)*.7))
cancer_train<-cancer_binary[dt,]
cancer_test<-cancer_binary[-dt,]
```

# EDA
```{r}
ggplot(data = cancer, mapping = aes(x = diagnosis)) +
  geom_bar()
```
The bar plot shows that there is a larger number of benign than malignant cancer. 

We divide the data into 3 categories according to their features.
```{r}
features_mean= cancer[2:10]
features_se= cancer[11:20]
features_worst= cancer[21:31]
```


```{r}
library(corrplot)
corrplot(cor(features_mean))
```

Major observations: 

- Radius_mean, perimeter_mean, and area_mean are highly correlated.

- Compactness_mean, concavity_mean and concave_points_mean are highly correlated.


```{r}
panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, color = "dodgerblue3", ...)
}

pairs(cancer[2:10], 
             method = "pearson", # correlation method
             digits=3,
             pch = '.',
             col=ifelse(cancer$diagnosis=="M", "red", "blue"),
             density = TRUE,  # show density plots
             ellipses = TRUE, # show correlation ellipses
             diag.panel =  panel.hist
             )
```

# Methodology 

# SVM

## Model Selection (Lasso penalized logistic regression)

```{r}
# Dumy code categorical predictor variables
x <- model.matrix(diagnosis~.-diagnosis_binary, cancer_train)[,-1]
# Convert the outcome (class) to a numerical variable
y <- ifelse(cancer_train$diagnosis == "M", 1, 0)
```


```{r}
library(glmnet)
# Find the best lambda using cross-validation
set.seed(123)
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
plot(cv.lasso)
```

```{r}
cv.lasso$lambda.min
# Final model with lambda.min
lasso.model <- glmnet(x, y, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.min)
coef(lasso.model)
```


## Linear Kernel SVM

We use the predictors selected by the LASSO penalized logistic regression as predictors for the support vector machine model:

If two predictors have high correlation, only use one of them:

```{r}
set.seed(1)
tune.out <- tune(svm, diagnosis_binary ~ concavity_mean+concave_points_mean+radius_se+texture_se+smoothness_se+compactness_se+
fractal_dimension_se+radius_worst+texture_worst+smoothness_worst+concavity_worst+concave_points_worst+symmetry_worst+fractal_dimension_worst, data = cancer_train, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1,
                                        1, 5, 10, 100)))
summary(tune.out)
best_linear_svm <- tune.out$best.model
summary(best_linear_svm)
```

```{r}
ypred <- predict(best_linear_svm, cancer_test)
table(predict = ypred, truth = cancer_test$diagnosis_binary)
classification_error <- 1- sum(ypred == cancer_test$diagnosis_binary)/length(ypred)
classification_error
```

The misclassification rate is 0.01754.


## Radial Kernel SVM

```{r}
set.seed(1)
tune.out <- tune(svm, diagnosis_binary ~ concavity_mean+concave_points_mean+radius_se+texture_se+smoothness_se+compactness_se+radius_worst+texture_worst+smoothness_worst+concavity_worst+concave_points_worst+symmetry_worst+fractal_dimension_worst, data = cancer_train,
                 kernel = "radial",
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out)
best_radial_svm <- tune.out$best.model
summary(best_radial_svm)
```

```{r}
ypred <- predict(best_radial_svm, cancer_test)
table(predict = ypred, truth = cancer_test$diagnosis_binary)
classification_error <- 1- sum(ypred == cancer_test$diagnosis_binary)/length(ypred)
```


# SVM Visualization

## Linear

```{r, echo=FALSE}
tune.out <- tune(svm, diagnosis_binary ~ concavity_mean+radius_se+texture_se+smoothness_se, data = cancer_train, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1,
                                        1, 5, 10, 100)))
best_linear_graph <- tune.out$best.model
```

```{r}
plot(best_linear_graph, cancer_train, radius_se~concavity_mean)

plot(best_linear_graph, cancer_train, texture_se~concavity_mean)
```

## Radial

```{r, echo=FALSE}
tune.out <- tune(svm, diagnosis_binary ~ concavity_mean+radius_se+texture_se+smoothness_se,
                 kernel = "radial", data = cancer_train,
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                               gamma = c(0.5, 1, 2, 3, 4)))
best_radial_graph <- tune.out$best.model
```

```{r}
plot(best_radial_graph, cancer_train, texture_se~concavity_mean)
plot(best_radial_graph, cancer_train, concavity_mean~texture_se)
```


## ROC (Linear SVM)

On train data: 

```{r}
library(pROC)
fitted <- attributes(predict(best_linear_svm, cancer_train,
                             decision.values=TRUE))$decision.values
ROC_svm1 <- roc(cancer_train$diagnosis_binary, fitted)

fitted <- attributes(predict(best_radial_svm, cancer_train,
                             decision.values=TRUE))$decision.values
ROC_svm2 <- roc(cancer_train$diagnosis_binary, fitted)
plot(ROC_svm1, col = "blue")
lines(ROC_svm2, col = "red")
legend("bottomright", legend = c("Linear", "Radial"), 
       col = c("blue", "red"), lty = 1)
```

On test data: 

```{r}
fitted <- attributes(predict(best_linear_svm, cancer_test,
                             decision.values=TRUE))$decision.values
ROC_svm1 <- roc(cancer_test$diagnosis_binary, fitted)

fitted <- attributes(predict(best_radial_svm, cancer_test,
                             decision.values=TRUE))$decision.values
ROC_svm2 <- roc(cancer_test$diagnosis_binary, fitted)
plot(ROC_svm1, col = "blue")
lines(ROC_svm2, col = "red")
legend("bottomright", legend = c("Linear", "Radial"), 
       col = c("blue", "red"), lty = 1)
```




# Random Forest

```{r}
rf <- randomForest(diagnosis_binary~.-diagnosis, data=cancer_train, proximity=TRUE)
importance(rf)
```

Select best mtry:

mean:
```{r}
set.seed(123)
rf_mean <- randomForest(diagnosis_binary~radius_mean + perimeter_mean + area_mean + concavity_mean + concave_points_mean, data = cancer_train, proximity=TRUE, mtry=4)
print(rf_mean)
```
```{r}
pred_mean <- predict(rf_mean, cancer_test)
confusionMatrix(pred_mean, cancer_test$diagnosis_binary)
```

```{r}
rf_worst <- randomForest(diagnosis_binary~radius_worst + perimeter_worst + area_worst + concave_points_worst + concavity_worst, data=cancer_train, proximity=TRUE)
print(rf_worst)
```

```{r}
pred_worst <- predict(rf_worst, cancer_test)
confusionMatrix(pred_worst, cancer_test$diagnosis_binary)
```

```{r}
ImpData <- as.data.frame(importance(rf_mean))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y= MeanDecreaseGini)) +
  geom_segment(aes(x=Var.Names, xend=Var.Names, y=0, yend=MeanDecreaseGini), color="skyblue") +
  theme_light() +
  coord_flip() +
  theme(
    legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  )
```
We observe that 'concave_points_mean' has the highest purity, so it is 

```{r}
mean_predictors <- cancer_train %>% select(radius_mean, perimeter_mean, area_mean, concavity_mean, concave_points_mean)
mtry <- tuneRF(mean_predictors, cancer_train$diagnosis_binary, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
```
```{r}
worst_predictors <- cancer_train %>% select(radius_worst, perimeter_worst, area_worst, concavity_worst, concave_points_worst)
mtryw <- tuneRF(worst_predictors, cancer_train$diagnosis_binary, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.w <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtryw)
print(best.w)
```

